{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMApegn/9RSct0IAh26PYRC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Agentic RAG\n","[Manaranjan Pradhan](www.manaranjanp.com)\n","\n","By the end of the tutorial we will have done the following:\n","\n","- Fetch and preprocess documents that will be used for retrieval.\n","- Index those documents for semantic search and create a retriever tool for the agent.\n","- Build an agentic RAG system that can decide when to use the retriever tool.\n","\n","Adopted from (https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/)\n","\n","### Libraries used\n","\n","  - LangChain\n","  - LangGraph\n","\n","### Install Required Libaries\n","\n"],"metadata":{"id":"Ki_y_9ZN25LW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSecCfxmhHIw"},"outputs":[],"source":["%%capture --no-stderr\n","%pip install -U --quiet langgraph langchain-groq langchain-community langchain-text-splitters unstructured langchain-huggingface langsmith"]},{"cell_type":"code","source":["import nest_asyncio\n","\n","nest_asyncio.apply()"],"metadata":{"id":"ckZMZqSLhK79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load the file and embed it\n","\n","Store the file converted_document.md in a folder called files under current directory."],"metadata":{"id":"q4-4H3C93nxb"}},{"cell_type":"code","source":["from langchain_community.document_loaders import DirectoryLoader\n","\n","loader = DirectoryLoader(\"./files\", glob=\"**/*.md\")\n","docs = loader.load()"],"metadata":{"id":"qlDkiz20hauL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(docs[0].page_content[:500])"],"metadata":{"id":"BMfm80QLiPRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n","    chunk_size=500, chunk_overlap=50\n",")\n","doc_splits = text_splitter.split_documents(docs)"],"metadata":{"id":"_mT3tgVhio08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.vectorstores import InMemoryVectorStore\n","from langchain_huggingface import HuggingFaceEmbeddings\n","\n","vectorstore = InMemoryVectorStore.from_documents(\n","    documents=doc_splits,\n","    embedding=HuggingFaceEmbeddings(model_name = 'BAAI/bge-large-en-v1.5',\n","                                    model_kwargs = {'device': 'cpu'})\n",")\n","retriever = vectorstore.as_retriever()"],"metadata":{"id":"H436SPNpiuOv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a Retriever"],"metadata":{"id":"dhZR1MHC31Co"}},{"cell_type":"code","source":["from langchain.tools.retriever import create_retriever_tool\n","\n","retriever_tool = create_retriever_tool(\n","    retriever,\n","    \"retrieve_quarterly_reports\",\n","    \"Search and return information about Infosys quarterly performance reports.\",\n",")"],"metadata":{"id":"u56RTdRDjY_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever_tool.invoke({\"query\": \"What is the revenue growth for the quarter?\"})"],"metadata":{"id":"YltCeqgTjR3O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Markdown, display\n","\n","display(Markdown(retriever_tool.invoke({\"query\": \"What is the revenue growth for the quarter?\"})))"],"metadata":{"id":"rUhTYyZU62Gy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create the Generator Model"],"metadata":{"id":"PYDEJ5kv34OU"}},{"cell_type":"code","source":["import os\n","from getpass import getpass\n","os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter your Groq API key: \")"],"metadata":{"id":"xrDCp_IYhZOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n","os.environ['LANGSMITH_TRACING'] = \"true\""],"metadata":{"id":"gcggdYuv47J8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"LANGSMITH_API_KEY\"] = getpass(\"Enter your LANGSMITH API key: \")"],"metadata":{"id":"OSPLcvek48lE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key in [\"LANGCHAIN_TRACING_V2\", \"LANGSMITH_API_KEY\", \"LANGCHAIN_PROJECT\"]:\n","    print(f\"{key} =\", os.getenv(key))"],"metadata":{"id":"0VlZaAO890lu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langgraph.graph import MessagesState\n","from langchain.chat_models import init_chat_model\n","from langchain_groq import ChatGroq\n","\n","#response_model = init_chat_model(\"groq:llama-3.3-70b-versatile\", temperature=0)\n","\n","response_model = ChatGroq(model=\"llama-3.3-70b-versatile\",\n","               temperature=0,\n","               max_tokens=256,\n","               max_retries=2)"],"metadata":{"id":"97oIBVa0kDlA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_query_or_respond(state: MessagesState):\n","    \"\"\"Call the model to generate a response based on the current state. Given\n","    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n","    \"\"\"\n","    response = (\n","        response_model\n","        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n","    )\n","    return {\"messages\": [response]}"],"metadata":{"id":"teI-EIp_kW2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input = {\n","    \"messages\": [\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"What is the revenue growth for the quarter?\",\n","        }\n","    ]\n","}"],"metadata":{"id":"NfJAxfUokffz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = generate_query_or_respond(input)"],"metadata":{"id":"z_DHmllPkwi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response.keys()"],"metadata":{"id":"sgJW6kPWlLBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response['messages']"],"metadata":{"id":"WdlgB-CLlNvh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Grade the retrieved Chunuks as relevant or not relevant"],"metadata":{"id":"We3PqWH837rn"}},{"cell_type":"code","source":["from pydantic import BaseModel, Field\n","from typing import Literal\n","\n","GRADE_PROMPT = (\n","    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n","    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n","    \"Here is the user question: {question} \\n\"\n","    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n","    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",")\n","\n","\n","class GradeDocuments(BaseModel):\n","    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n","\n","    binary_score: str = Field(\n","        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n","    )\n","\n","grader_model = init_chat_model(\"groq:llama-3.3-70b-versatile\", temperature=0)\n","\n","def grade_documents(\n","    state: MessagesState,\n",") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n","    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n","    question = state[\"messages\"][0].content\n","    context = state[\"messages\"][-1].content\n","\n","    prompt = GRADE_PROMPT.format(question=question, context=context)\n","    response = (\n","        grader_model\n","        .with_structured_output(GradeDocuments).invoke(\n","            [{\"role\": \"user\", \"content\": prompt}]\n","        )\n","    )\n","    score = response.binary_score\n","\n","    print(\"###############################################\")\n","    print(f\"Score from grade_documents: {score}\")\n","    print(\"###############################################\")\n","\n","    if score == \"yes\":\n","        return \"generate_answer\"\n","    else:\n","        return \"rewrite_question\""],"metadata":{"id":"J6cSmDqrlQAY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a Prompt Rewriter"],"metadata":{"id":"BDHzlzh34Bw9"}},{"cell_type":"code","source":[" REWRITE_PROMPT = (\n","    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n","    \"Here is the initial question:\"\n","    \"\\n ------- \\n\"\n","    \"{question}\"\n","    \"\\n ------- \\n\"\n","    \"Formulate an improved question:\"\n",")\n","\n","\n","def rewrite_question(state: MessagesState):\n","    \"\"\"Rewrite the original user question.\"\"\"\n","    messages = state[\"messages\"]\n","    question = messages[0].content\n","    prompt = REWRITE_PROMPT.format(question=question)\n","    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n","    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"],"metadata":{"id":"9nf1QS8ulxOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Configure the Generator Node"],"metadata":{"id":"81Yeuh2p4Hhu"}},{"cell_type":"code","source":["GENERATE_PROMPT = (\n","    \"You are an assistant for question-answering tasks. \"\n","    \"Use the following pieces of retrieved context to answer the question. \"\n","    \"If you don't know the answer, just say that you don't know. \"\n","    \"Use three sentences maximum and keep the answer concise.\\n\"\n","    \"Question: {question} \\n\"\n","    \"Context: {context}\"\n",")\n","\n","\n","def generate_answer(state: MessagesState):\n","    \"\"\"Generate an answer.\"\"\"\n","    question = state[\"messages\"][0].content\n","    context = state[\"messages\"][-1].content\n","    prompt = GENERATE_PROMPT.format(question=question, context=context)\n","    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n","    return {\"messages\": [response]}"],"metadata":{"id":"Bwcu5G3Xlyxz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create the Agentic RAG Graph"],"metadata":{"id":"cb2zbkfl4M1i"}},{"cell_type":"code","source":["from langgraph.graph import StateGraph, START, END\n","from langgraph.prebuilt import ToolNode\n","from langgraph.prebuilt import tools_condition\n","\n","workflow = StateGraph(MessagesState)\n","\n","# Define the nodes we will cycle between\n","workflow.add_node(generate_query_or_respond)\n","workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n","workflow.add_node(rewrite_question)\n","workflow.add_node(generate_answer)\n","\n","workflow.add_edge(START, \"generate_query_or_respond\")\n","\n","# Decide whether to retrieve\n","workflow.add_conditional_edges(\n","    \"generate_query_or_respond\",\n","    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n","    tools_condition,\n","    {\n","        # Translate the condition outputs to nodes in our graph\n","        \"tools\": \"retrieve\",\n","        END: END,\n","    },\n",")\n","\n","# Edges taken after the `action` node is called.\n","workflow.add_conditional_edges(\n","    \"retrieve\",\n","    # Assess agent decision\n","    grade_documents,\n",")\n","workflow.add_edge(\"generate_answer\", END)\n","workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n","\n","# Compile\n","graph = workflow.compile()"],"metadata":{"id":"ahl6ryjwl1A6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Image, display\n","\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"id":"BufG8040l3FR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Invoke the Graph"],"metadata":{"id":"6oCtETEd4QPQ"}},{"cell_type":"code","source":["for chunk in graph.stream(\n","    {\n","        \"messages\": [\n","            {\n","                \"role\": \"user\",\n","                \"content\": \"What is the revenue growth for the quarter?\",\n","            }\n","        ]\n","    }\n","):\n","    for node, update in chunk.items():\n","        print(\"Update from node\", node)\n","        update[\"messages\"][-1].pretty_print()\n","        print(\"\\n\\n\")"],"metadata":{"id":"PlmgbPESl51A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iBlcycE34ew0"},"execution_count":null,"outputs":[]}]}